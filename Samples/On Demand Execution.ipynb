{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're going to build a small version of what `main.py` does. The goal is to \n",
    "*   Connect to the database\n",
    "*   Create the schema defined\n",
    "*   Create the `dim_staff` table\n",
    "*   Extract that table\n",
    "*   Transform that table\n",
    "*   Load the table into the data warehouse\n",
    "*   Teardown the connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "from psycopg import Cursor\n",
    "from pypika import PostgreSQLQuery\n",
    "from pypika import Schema, Column, PostgreSQLQuery\n",
    "from dexxy.common.tasks import Task\n",
    "from dexxy.common.workflows import Pipeline\n",
    "from dexxy.common.plotting import plot_dag\n",
    "from dexxy.database.postgres import PostgresClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Parameters ###################\n",
    "# Neccessary for connecting to the database. \n",
    "\n",
    "databaseConfig = \"../config/database.ini\"\n",
    "section = 'postgresql'\n",
    "dw = Schema('dssa')\n",
    "dvd = Schema('public')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Table Definitions ################\n",
    "# These are some generic builds for our star-schema. For visual reference refer to the star-schema.jpg. \n",
    "DIM_STAFF = (\n",
    "    Column('sk_staff', 'INT', False),\n",
    "    Column('name', 'VARCHAR(100)', False),\n",
    "    Column('email', 'VARCHAR(100)', False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Functions ####################\n",
    "def createCursor(path:str, section:str) -> Cursor:\n",
    "    client = PostgresClient()\n",
    "    conn = client.connect_from_config(path, section, autocommit=True)\n",
    "    cursor = conn.cursor()\n",
    "    return cursor\n",
    "\n",
    "# The code below assumes you'll also have a global cursor variable for connecting to the database.\n",
    "cursor = createCursor(databaseConfig, section)\n",
    "\n",
    "def createSchema(cursor: Cursor, schemaName: str) -> Cursor:\n",
    "    q = f\"CREATE SCHEMA IF NOT EXISTS {schemaName};\"\n",
    "    cursor.execute(q)\n",
    "    return cursor\n",
    "\n",
    "def createTable(cursor:Cursor, tableName:str, definition:tuple, primaryKey:str=None, foreignKeys:list=None, referenceTables:list=None) -> None: \n",
    "    ddl = PostgreSQLQuery \\\n",
    "        .create_table(tableName) \\\n",
    "        .if_not_exists() \\\n",
    "        .columns(*definition)\n",
    "        \n",
    "    if primaryKey is not None:\n",
    "        ddl = ddl.primary_key(primaryKey)\n",
    "        \n",
    "    if foreignKeys is not None:\n",
    "        for idx, key in enumerate(foreignKeys):\n",
    "            ddl.foreign_key(\n",
    "                columns=key,\n",
    "                reference_table = referenceTables[idx],\n",
    "                reference_columns = key\n",
    "            )\n",
    "            \n",
    "    ddl = ddl.get_sql()\n",
    "    \n",
    "    cursor.execute(ddl)\n",
    "    return    \n",
    "\n",
    "def readData(tableName:str, columns:tuple) -> pd.DataFrame:\n",
    "    query = PostgreSQLQuery \\\n",
    "        .from_(tableName) \\\n",
    "        .select(*columns) \\\n",
    "        .get_sql()\n",
    "    res = cursor.execute(query)\n",
    "    data = res.fetchall()\n",
    "    \n",
    "    col_names = []\n",
    "    \n",
    "    for names in res.description:\n",
    "        col_names.append(names[0])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=col_names)\n",
    "    return df\n",
    "\n",
    "def loadData(df:pd.DataFrame, target:str):\n",
    "    data = tuple(df.itertuples(index=False, name=None))\n",
    "    query = PostgreSQLQuery \\\n",
    "        .into(target) \\\n",
    "        .insert(*data) \\\n",
    "        .get_sql()\n",
    "    cursor.execute(query)\n",
    "    return \n",
    "\n",
    "def buildDimStaff(staff_df:pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n",
    "    staff_df.rename(columns={'staff_id': 'sk_staff'}, inplace=True)\n",
    "    staff_df['name'] = staff_df.first_name + \" \" + staff_df.last_name\n",
    "    dim_staff = staff_df[['sk_staff', 'name', 'email']].copy()\n",
    "    dim_staff.drop_duplicates(inplace=True)\n",
    "    return dim_staff\n",
    "\n",
    "def tearDown(*args, **kwargs) -> None:\n",
    "    cursor.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what would be placed inside `def main():`\n",
    "\n",
    "This is how you would build define and build Tasks/Pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    setup = Pipeline(\n",
    "        steps=[\n",
    "            Task(createCursor,\n",
    "                kwargs={'path': databaseConfig, 'section': section},\n",
    "                dependsOn=None,\n",
    "                name='createCursor'\n",
    "            ),\n",
    "            Task(createSchema,\n",
    "                kwargs={\"schemaName\": dw._name},\n",
    "                dependsOn=['createCursor'],\n",
    "                name='createSchema'\n",
    "            ),\n",
    "            Task(createTable,\n",
    "                kwargs={'tableName': dw.staff, 'primaryKey': 'sk_staff', 'definition':DIM_STAFF},\n",
    "                dependsOn=['createSchema'],\n",
    "                name='createDimStaff'\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    extract = Pipeline(\n",
    "        steps=[Task(readData,\n",
    "                kwargs={'tableName': dvd.staff,'columns': ('staff_id', 'first_name', 'last_name', 'email')},\n",
    "                dependsOn=['createFactRentals'],\n",
    "                name='extractStaff'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    transform = Pipeline(\n",
    "        steps=[\n",
    "            Task(buildDimStaff,\n",
    "                dependsOn=['extractStaff', 'transformCustomer'],\n",
    "                name='transformStaff'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    load = Pipeline(\n",
    "        steps=[\n",
    "            Task(loadData,\n",
    "                dependsOn=['transformStaff'],\n",
    "                kwargs={'target': dw.staff},\n",
    "                name='loadStaff'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Creates a DAG for tear down tasks and closing out any open connections to the database\n",
    "    teardown = Pipeline(\n",
    "        steps =[\n",
    "            Task(tearDown,\n",
    "                dependsOn= [load],\n",
    "                name='tearDown',\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # We merge all the above Pipelines into a single Pipeline containing all Tasks to be added to the DAG.\n",
    "    workflow = Pipeline(\n",
    "        steps=[\n",
    "            setup,\n",
    "            extract,\n",
    "            transform,\n",
    "            load,\n",
    "            teardown\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this ETL pipeline, you will need to run a few functions after the workflow definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ COMPILATION ============================ #\n",
    "# This section composes the DAG from the provided Tasks \n",
    "workflow.compose()\n",
    "\n",
    "# ============================ ENQUEUE ============================ #\n",
    "# This section uses the .collect() method which enqueues all tasks in the DAG to a task FIFO queue in topological order \n",
    "workflow.collect()\n",
    "\n",
    "# ============================ EXECUTION ============================ #\n",
    "# Runs the workflow locally using a single worker\n",
    "workflow.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
